# To-do list

## Implementations

- Basic MNIST

## Features

- softmax (NormalizationLayer/Function)
- CombinedLayer: Linear + NonLinear
- Add minibatches back in
- Shuffle minibatches each iteration
- Dropout
- Convolutions
- Recurrent networks
- GANs
- Momentum

## Refactors

- Refactor layer updates in terms of matrix multiplication
- Don't duplicate storing previous layer activations as next layer inputs

## Tests

- Fix layer tests

## Nice-to-have

- Shared GHC pragma for options like -fno-warn-tabs
- shellcheck for scripts
